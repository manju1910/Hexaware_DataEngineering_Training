{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aO0xwCjd11Lt",
        "outputId": "e618ad57-810d-4d67-c0ec-6ddb39da3d1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.2.tar.gz (317.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.2-py2.py3-none-any.whl size=317812365 sha256=c35dbb1a868cc2d2dd877d742e7e5cecba0b6046d42f85312d3cb65537f20965\n",
            "  Stored in directory: /root/.cache/pip/wheels/34/34/bd/03944534c44b677cd5859f248090daa9fb27b3c8f8e5f49574\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.2\n"
          ]
        }
      ],
      "source": [
        "pip install pyspark\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Set Up Spark Streaming\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "A1seUJn8YiSS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import yfinance as yf\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import Row\n",
        "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, LongType\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Define the stock symbol (e.g., Apple stock)\n",
        "SYMBOL = 'CTS'  # Replace with the stock symbol of your choice\n",
        "\n",
        "# Fetch stock data using Yahoo Finance (yfinance)\n",
        "def fetch_stock_data_yfinance():\n",
        "    stock = yf.Ticker(SYMBOL)\n",
        "    hist = stock.history(period=\"1d\", interval=\"1m\")  # Fetch 1-minute data for today\n",
        "\n",
        "    # Rename columns and convert data to a list of dictionaries\n",
        "    hist = hist.rename(columns={\n",
        "        'Date': 'trade_date',\n",
        "        'Open': 'open_price',\n",
        "        'High': 'high_price',\n",
        "        'Low': 'low_price',\n",
        "        'Close': 'close_price',\n",
        "        'Volume': 'volume'\n",
        "    })\n",
        "\n",
        "    return [\n",
        "        {\n",
        "            'symbol': SYMBOL,\n",
        "            'timestamp': str(index),\n",
        "            'low': float(row['low_price']),\n",
        "            'open': float(row['open_price']),\n",
        "            'close': float(row['close_price']),# Convert to native float\n",
        "            'volume': int(row['volume'])  # Convert to native int\n",
        "        }\n",
        "        for index, row in hist.iterrows()\n",
        "    ]\n",
        "\n",
        "# Convert API data into Spark DataFrame rows\n",
        "def to_spark_row(stock_data):\n",
        "    return Row(\n",
        "        symbol=stock_data['symbol'],\n",
        "        open=stock_data['open'],\n",
        "        low=stock_data['low'],\n",
        "        close=stock_data['close'],\n",
        "        volume=stock_data['volume'],\n",
        "        timestamp=stock_data['timestamp']\n",
        "    )\n",
        "\n",
        "# Initialize Spark session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"RealTimeStockDataYahooFinance\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Define the schema for stock data\n",
        "schema = StructType([\n",
        "    StructField(\"symbol\", StringType(), True),\n",
        "     StructField(\"low\", DoubleType(), True),\n",
        "    StructField(\"open\", DoubleType(), True),\n",
        "    StructField(\"close\", DoubleType(), True),\n",
        "    StructField(\"volume\", LongType(), True),\n",
        "    StructField(\"timestamp\", StringType(), True)\n",
        "])\n",
        "\n",
        "# Fetch and process data with stopping condition\n",
        "def stream_data_to_spark(max_batches=5):\n",
        "    batch_count = 0\n",
        "    while batch_count < max_batches:  # Stop after 5 batches (or any number you choose)\n",
        "        data = fetch_stock_data_yfinance()  # Fetch stock data using Yahoo Finance API\n",
        "        if data:\n",
        "            rows = [to_spark_row(d) for d in data]  # Convert to rows\n",
        "            df = spark.createDataFrame(rows, schema)  # Create Spark DataFrame\n",
        "\n",
        "            # Example: Show the DataFrame or replace this with any processing logic\n",
        "            df.show()\n",
        "\n",
        "        else:\n",
        "            print(\"No data fetched. Retrying...\")\n",
        "\n",
        "        batch_count += 1  # Increment the batch counter\n",
        "        time.sleep(60)  # Wait for 60 seconds before fetching the next batch of data\n",
        "\n",
        "    print(\"Streaming process completed after\", batch_count, \"batches.\")\n",
        "\n",
        "# Start the data streaming process\n",
        "stream_data_to_spark(max_batches=1)  # Run for 1 batch for testing\n",
        "\n"
      ],
      "metadata": {
        "id": "mdgKPef8KusO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56033777-3900-42ff-bdea-26c3eda085fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+------------------+------------------+------------------+------+--------------------+\n",
            "|symbol|               low|              open|             close|volume|           timestamp|\n",
            "+------+------------------+------------------+------------------+------+--------------------+\n",
            "|   CTS|46.970001220703125| 46.78499984741211| 47.08000183105469|     0|2024-09-18 09:30:...|\n",
            "|   CTS|  46.8650016784668|  46.8650016784668|  46.8650016784668|   112|2024-09-18 09:35:...|\n",
            "|   CTS| 46.82500076293945| 46.82500076293945|  46.8650016784668|   575|2024-09-18 09:45:...|\n",
            "|   CTS|46.904998779296875|46.904998779296875|46.904998779296875|   461|2024-09-18 09:46:...|\n",
            "|   CTS| 46.96500015258789| 46.96500015258789| 46.96500015258789|   590|2024-09-18 10:03:...|\n",
            "|   CTS| 46.96500015258789| 46.96500015258789| 46.96500015258789|   199|2024-09-18 10:08:...|\n",
            "|   CTS|46.790000915527344|46.790000915527344|46.810001373291016|   729|2024-09-18 10:14:...|\n",
            "|   CTS| 46.82500076293945| 46.82500076293945| 46.82500076293945|   653|2024-09-18 10:20:...|\n",
            "|   CTS|  46.7400016784668| 46.72999954223633|46.744998931884766|  1624|2024-09-18 10:30:...|\n",
            "|   CTS| 46.48500061035156| 46.48500061035156| 46.48500061035156|   930|2024-09-18 10:36:...|\n",
            "|   CTS| 46.61000061035156| 46.61000061035156| 46.61000061035156|   975|2024-09-18 10:50:...|\n",
            "|   CTS| 46.67499923706055| 46.67499923706055| 46.67499923706055|   227|2024-09-18 10:55:...|\n",
            "|   CTS| 46.59000015258789| 46.59000015258789| 46.59000015258789|   237|2024-09-18 10:58:...|\n",
            "|   CTS| 46.67499923706055| 46.67499923706055| 46.70000076293945|   434|2024-09-18 10:59:...|\n",
            "|   CTS|  46.7400016784668|  46.7400016784668|  46.7400016784668|   164|2024-09-18 11:00:...|\n",
            "|   CTS|  46.7400016784668|  46.7400016784668|  46.7400016784668|   371|2024-09-18 11:01:...|\n",
            "|   CTS| 46.79499816894531| 46.79499816894531| 46.79499816894531|   484|2024-09-18 11:03:...|\n",
            "|   CTS|             46.75|  46.7400016784668|             46.75|   943|2024-09-18 11:13:...|\n",
            "|   CTS|46.755001068115234|46.755001068115234| 46.81999969482422|  1200|2024-09-18 11:14:...|\n",
            "|   CTS| 46.81999969482422| 46.81999969482422| 46.81999969482422|   432|2024-09-18 11:17:...|\n",
            "+------+------------------+------------------+------------------+------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "Streaming process completed after 1 batches.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Anomaly Detection\n"
      ],
      "metadata": {
        "id": "7yxhoccdYYsk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, avg, stddev, abs\n",
        "\n",
        "# Function to perform anomaly detection\n",
        "def detect_anomalies(df):\n",
        "    # Define window specifications\n",
        "    window_spec = Window.orderBy(\"timestamp\").rowsBetween(-5, 0)  # 5-minute window for moving averages\n",
        "\n",
        "    # Calculate moving average and standard deviation for the 'close' price\n",
        "    df = df.withColumn(\"moving_avg\", avg(col(\"close\")).over(window_spec))\n",
        "    df = df.withColumn(\"moving_stddev\", stddev(col(\"close\")).over(window_spec))\n",
        "\n",
        "    # Define thresholds for anomaly detection\n",
        "    threshold_price = 0.05  # 5% deviation from moving average\n",
        "    threshold_volume = 1.5  # 1.5 times the average volume\n",
        "\n",
        "    # Detect anomalies based on price deviation and volume spikes\n",
        "    df = df.withColumn(\"price_anomaly\", abs(col(\"close\") - col(\"moving_avg\")) > (col(\"moving_avg\") * threshold_price))\n",
        "    df = df.withColumn(\"volume_anomaly\", col(\"volume\") > (avg(col(\"volume\")).over(window_spec) * threshold_volume))\n",
        "\n",
        "    # Filter rows with anomalies\n",
        "    anomalies = df.filter(col(\"price_anomaly\") | col(\"volume_anomaly\"))\n",
        "\n",
        "    return anomalies\n",
        "\n",
        "def stream_data_to_spark(max_batches=5):\n",
        "    batch_count = 0\n",
        "    while batch_count < max_batches:\n",
        "        data = fetch_stock_data_yfinance()\n",
        "        if data:\n",
        "            rows = [to_spark_row(d) for d in data]\n",
        "            df = spark.createDataFrame(rows, schema)\n",
        "\n",
        "            # Perform anomaly detection\n",
        "            anomalies = detect_anomalies(df)\n",
        "\n",
        "            # Show anomalies\n",
        "            if anomalies.count() > 0:\n",
        "                anomalies.show()\n",
        "            else:\n",
        "                print(\"No anomalies detected in this batch.\")\n",
        "\n",
        "        else:\n",
        "            print(\"No data fetched. Retrying...\")\n",
        "\n",
        "        batch_count += 1\n",
        "        time.sleep(60)\n",
        "\n",
        "    print(\"Streaming process completed after\", batch_count, \"batches.\")\n",
        "\n",
        "# Start the data streaming process\n",
        "stream_data_to_spark(max_batches=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G8UxRXLvzMik",
        "outputId": "193932ef-7b38-44c6-c8fd-65f475847248"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+------------------+------------------+------------------+------+--------------------+------------------+--------------------+-------------+--------------+\n",
            "|symbol|               low|              open|             close|volume|           timestamp|        moving_avg|       moving_stddev|price_anomaly|volume_anomaly|\n",
            "+------+------------------+------------------+------------------+------+--------------------+------------------+--------------------+-------------+--------------+\n",
            "|   CTS|  46.8650016784668|  46.8650016784668|  46.8650016784668|   112|2024-09-18 09:35:...| 46.97250175476074|  0.1520280658510399|        false|          true|\n",
            "|   CTS| 46.82500076293945| 46.82500076293945|  46.8650016784668|   575|2024-09-18 09:45:...|46.936668395996094| 0.12413039597242927|        false|          true|\n",
            "|   CTS|46.904998779296875|46.904998779296875|46.904998779296875|   461|2024-09-18 09:46:...| 46.92875099182129| 0.10258156734188123|        false|          true|\n",
            "|   CTS| 46.96500015258789| 46.96500015258789| 46.96500015258789|   590|2024-09-18 10:03:...| 46.93600082397461| 0.09030522576237918|        false|          true|\n",
            "|   CTS|46.790000915527344|46.790000915527344|46.810001373291016|   729|2024-09-18 10:14:...| 46.89583396911621| 0.06151489318400009|        false|          true|\n",
            "|   CTS|  46.7400016784668| 46.72999954223633|46.744998931884766|  1624|2024-09-18 10:30:...|46.869166692097984| 0.09002315224019261|        false|          true|\n",
            "|   CTS| 46.79499816894531| 46.79499816894531| 46.79499816894531|   484|2024-09-18 11:03:...| 46.70666694641113| 0.07026127837637776|        false|          true|\n",
            "|   CTS|             46.75|  46.7400016784668|             46.75|   943|2024-09-18 11:13:...| 46.71916707356771| 0.07017211246592694|        false|          true|\n",
            "|   CTS|46.755001068115234|46.755001068115234| 46.81999969482422|  1200|2024-09-18 11:14:...|46.757500330607094| 0.04309784055430068|        false|          true|\n",
            "|   CTS|46.720001220703125|46.720001220703125|46.720001220703125|   794|2024-09-18 11:35:...| 46.80833371480306|0.046654397357440154|        false|          true|\n",
            "|   CTS|46.599998474121094|46.599998474121094|46.599998474121094|  1202|2024-09-18 11:47:...| 46.71666717529297| 0.08304673292214376|        false|          true|\n",
            "|   CTS|46.529998779296875|46.529998779296875|46.529998779296875|   974|2024-09-18 12:09:...|46.600833892822266| 0.08505460475076618|        false|          true|\n",
            "+------+------------------+------------------+------------------+------+--------------------+------------------+--------------------+-------------+--------------+\n",
            "\n",
            "Streaming process completed after 1 batches.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PoD1srbI5wcJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}