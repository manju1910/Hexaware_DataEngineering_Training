{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# @title Assignment: Delta Lake Concepts\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "8OG7sy_kLCAd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2500fafa-ee7f-4897-de64-c46db6b46f56",
        "id": "A9t5nrRVVSuC"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.2.tar.gz (317.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.2-py2.py3-none-any.whl size=317812365 sha256=3239ddee45ff3608cb5e3002a0e3d942104693b27a8284c9a04d2b0dd10fc39a\n",
            "  Stored in directory: /root/.cache/pip/wheels/34/34/bd/03944534c44b677cd5859f248090daa9fb27b3c8f8e5f49574\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.2\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create Spark session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"DeltaLakeAssignment\") \\\n",
        "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
        "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Import Delta Lake packages\n",
        "from delta.tables import *\n",
        "\n",
        "# Confirm Spark session is active\n",
        "spark\n",
        "\n"
      ],
      "metadata": {
        "id": "OTfWKKEXVSuD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Task 1: Creating Delta Table using Three Methods\n",
        "\n",
        "# 1.1\n",
        "# Load the CSV dataset (Employees)\n",
        "employees_df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(\"/path_to/employees.csv\")\n",
        "\n",
        "# Load the JSON dataset (Products)\n",
        "products_df = spark.read.format(\"json\").load(\"/path_to/products.json\")\n",
        "\n",
        "\n",
        "#1.2\n",
        "\n",
        "# Write the DataFrame as a Delta Table (Employees)\n",
        "employees_df.write.format(\"delta\").mode(\"overwrite\").save(\"/path_to/delta/employees_delta\")\n",
        "\n",
        "# Write the DataFrame as a Delta Table (Products)\n",
        "products_df.write.format(\"delta\").mode(\"overwrite\").save(\"/path_to/delta/products_delta\")\n",
        "\n",
        "# Create Delta table for Employees using SQL\n",
        "spark.sql(\"\"\"\n",
        "    CREATE TABLE delta_employees\n",
        "    USING DELTA\n",
        "    AS SELECT * FROM parquet.`/path_to/delta/employees_delta`\n",
        "\"\"\")\n",
        "\n",
        "# Create Delta table for Products using SQL\n",
        "spark.sql(\"\"\"\n",
        "    CREATE TABLE delta_products\n",
        "    USING DELTA\n",
        "    AS SELECT * FROM parquet.`/path_to/delta/products_delta`\n",
        "\"\"\")\n"
      ],
      "metadata": {
        "id": "3ew2ki9XVSuD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 2: Merge and Upsert (Slowly Changing Dimension - SCD)\n",
        "\n",
        "\n",
        "\n",
        "#2.1  Load the Delta table for employees created in Task 1.\n",
        "\n",
        "employee_df_updates = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"/FileStore/employee_updates.csv\")\n",
        "employee_df_updates.write.format(\"delta\").mode(\"overwrite\").save(\"/delta/employee_updates\")\n",
        "\n",
        "\n",
        "employee_df_updates = spark.read.format(\"delta\").load(\"/delta/employee_updates\")\n",
        "\n",
        "\n",
        "spark.sql(\"\"\"\n",
        "    CREATE TABLE delta_employee_updates\n",
        "    USING DELTA\n",
        "    AS SELECT * FROM parquet.`/path_to/delta/employees_updates`\n",
        "\"\"\")\n",
        "\n",
        "#2.2  Merge the new employee data into the employees Delta table.\n",
        "\n",
        "spark.sql(\"\"\"\n",
        "    MERGE INTO delta_employees AS target\n",
        "    USING employee_updates AS source\n",
        "    ON target.EmployeeID = source.EmployeeID\n",
        "    WHEN MATCHED THEN UPDATE SET target.Salary = source.Salary, target.Department = source.Department\n",
        "    WHEN NOT MATCHED THEN INSERT (EmployeeID, Name, Department, JoiningDate, Salary)\n",
        "    VALUES (source.EmployeeID, source.Name, source.Department, source.JoiningDate, source.Salary)\n",
        "\"\"\")\n"
      ],
      "metadata": {
        "id": "4K_5oiE0VSuE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Task 3: Internals of Delta Table\n",
        "\n",
        "#Check the transaction history of the table.\n",
        "spark.sql(\"DESCRIBE HISTORY delta_employee\").show(truncate=False)\n",
        "\n",
        "#Perform Time Travel and retrieve the table before the previous merge operation.\n",
        "\n",
        "version_before_merge = delta_employee_table.history().filter(\"operation = 'MERGE'\").select(\"version\").first()[0] - 1\n",
        "\n",
        "# Time Travel: Retrieve data before the previous merge\n",
        "previous_version_df = spark.read.format(\"delta\").option(\"versionAsOf\", version_before_merge).load(\"/path_to/delta/employees_delta\")\n",
        "previous_version_df.show()\n"
      ],
      "metadata": {
        "id": "GI2cUN2LVSuE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Task 4: Optimize Delta Table\n",
        "\n",
        "#4.1. Optimize the employees Delta table for better performance.\n",
        "spark.sql(\"OPTIMIZE delta_employee_table\")\n",
        "\n",
        "#4.2. Use Z-ordering on the Department column for improved query performance.\n",
        "spark.sql(\"\"\"\n",
        "\n",
        "OPTIMIZE delta_employee_table ZORDER BY Department \"\"\")"
      ],
      "metadata": {
        "id": "50NpdS1nVSuF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Task 5: Time Travel with Delta Table\n",
        "\n",
        "#5.1. Retrieve the employees Delta table as it was before the last merge.\n",
        "time_travel_df = spark.read.format(\"delta\").option(\"versionAsOf\", version_before_merge).load(\"/path_to/delta/employees_delta\")\n",
        "time_travel_df.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "zM74UfaVVSuF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Task 6: Vacuum Delta Table\n",
        "\n",
        "#6.1. Use the vacuum operation on the employees Delta table to remove old versions and free up disk space.\n",
        "#6.2. Set the retention period to 7 days and ensure that old files are deleted.\n",
        "\n",
        "spark.sql(\"\"\"\n",
        "\n",
        "VACUUM delta_employee_table RETAIN 168 HOURS \"\"\")"
      ],
      "metadata": {
        "id": "S3j-J4wsVSuF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Assignment: Structured Streaming and Transformations on Streams\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "2QffcZFNVoG1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dbutils.fs.cp(\"file:/content/sample_data/sales_data.csv\", \"dbfs:/FileStore/streaming/input/sales_data.csv\")\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DateType\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "#Initialize SparkSession\n",
        "\n",
        "spark =SparkSession.builder\\\n",
        "     .appName(\"StructuredStreamingExample\") \\\n",
        "     .getOrCreate()"
      ],
      "metadata": {
        "id": "nNgjd4eTVtrE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TASK 1\n",
        "#Define the schema for the CSV data\n",
        "sales_schema = \"TransactionID INT, TransactionDate STRING, ProductID INT, Quantity INT, Price DOUBLE\"\n",
        "\n",
        "#Read streaming data from CSV files\n",
        "df_sales_stream =spark.readStream\\\n",
        "          .format(\"csv\") \\\n",
        "          .option(\"header\", \"true\") \\\n",
        "          .schema (sales_schema)\\\n",
        "          .load(\"dbfs:/Filestore/streaming/input/\")\n",
        "\n",
        "query = spark.writeStream \\\n",
        "    .outputMode(\"append\") \\\n",
        "    .format(\"console\") \\\n",
        "    .start()\n",
        "query.awaitTermination()"
      ],
      "metadata": {
        "id": "cvNAk-AfXnlF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TASK 2\n",
        "transformed_df = spark.withColumn(\"TotalAmount\", col(\"Quantity\") * col(\"Price\")) \\\n",
        "                   .filter(col(\"Quantity\") > 1)\n",
        "\n",
        "memory_query = transformed_df.writeStream \\\n",
        "    .outputMode(\"append\") \\\n",
        "    .format(\"memory\") \\\n",
        "    .queryName(\"transformed_data\") \\\n",
        "    .start()\n",
        "\n",
        "memory_query.awaitTermination()\n",
        "spark.sql(\"SELECT * FROM transformed_data\").show()"
      ],
      "metadata": {
        "id": "XO00FTj8XqUZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TASK 3\n",
        "aggregated_df = transformed_df.groupBy(\"ProductID\") \\\n",
        "                              .agg({\"TotalAmount\": \"sum\"}) \\\n",
        "                              .withColumnRenamed(\"sum(TotalAmount)\", \"TotalSales\")\n",
        "\n",
        "aggregated_query = aggregated_df.writeStream \\\n",
        "    .outputMode(\"update\") \\\n",
        "    .format(\"console\") \\\n",
        "    .start()\n",
        "aggregated_query.awaitTermination()\n"
      ],
      "metadata": {
        "id": "2EE4xuDzXs8H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TASK 4\n",
        "query_to_parquet = (aggregated_query.writeStream\n",
        "                                .format(\"parquet\")\n",
        "                                .outputMode(\"append\")\n",
        "                                .option(\"path\", \"/output/stream_parquet/\")\n",
        "                                .option(\"checkpointLocation\", \"/output/checkpoints/\")\n",
        "                                .start())\n"
      ],
      "metadata": {
        "id": "Rws5gOI0Xv05"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TASK 5\n",
        "watermark_df = transformed_df.withWatermark(\"TransactionDate\", \"1 day\")\n",
        "watermarked_aggregated_df = watermark_df.groupBy(\"ProductID\") \\\n",
        "                                        .agg({\"TotalAmount\": \"sum\"}) \\\n",
        "                                        .withColumnRenamed(\"sum(TotalAmount)\", \"TotalSales\")"
      ],
      "metadata": {
        "id": "2sST8R6HXye6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#TASK 6\n",
        "product_schema = StructType([\n",
        "    StructField(\"ProductID\", StringType(), True),\n",
        "    StructField(\"ProductName\", StringType(), True),\n",
        "    StructField(\"Category\", StringType(), True)\n",
        "])\n",
        "\n",
        "products_df = spark.readStream \\\n",
        "    .schema(product_schema) \\\n",
        "    .csv(\"file:/content/sample_data/sales_data.csv\")\n",
        "\n",
        "joined_df = transformed_df.join(products_df, on=\"ProductID\", how=\"inner\")\n",
        "\n",
        "joined_query = joined_df.writeStream \\\n",
        "    .outputMode(\"append\") \\\n",
        "    .format(\"console\") \\\n",
        "    .start()\n",
        "joined_query.awaitTermination()\n"
      ],
      "metadata": {
        "id": "ZuhOpSeZXzJS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#TASK 7\n",
        "query.stop()\n",
        "query_restarted = (df_sales_stream.writeStream\n",
        "                               .format(\"parquet\")\n",
        "                               .outputMode(\"append\")\n",
        "                               .option(\"path\", \"/output/stream_restarted/\")\n",
        "                               .option(\"checkpointLocation\", \"/output/checkpoints/\")\n",
        "                               .start())"
      ],
      "metadata": {
        "id": "JnaDlCXVXzlP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Assignment: Creating a Complete ETL Pipeline using Delta Live Tables\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "ModyzsLSX7eD"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "\n",
        "#Task 1: Create an ETL Pipeline using DLT (Python)\n",
        "\n",
        "# 1: Read the source data from CSV/Parquet\n",
        "source_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"/path/to/orders.csv\")\n",
        "\n",
        "# 2: Transform the data\n",
        "# Add a new column 'TotalAmount' by multiplying 'Quantity' by 'Price'\n",
        "transformed_df = source_df.withColumn(\"TotalAmount\", col(\"Quantity\") * col(\"Price\"))\n",
        "\n",
        "# Filter where Quantity is greater than 1\n",
        "transformed_filtered_df = transformed_df.filter(col(\"Quantity\") > 1)\n",
        "\n",
        "# 3: Write the transformed data to a Delta table\n",
        "transformed_filtered_df.write.format(\"delta\").mode(\"overwrite\").save(\"/path/to/delta_orders_table\")\n",
        "\n"
      ],
      "metadata": {
        "id": "yccyY7UYYNT9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 2: Create an ETL Pipeline using DLT (SQL)\n",
        "# 1: Create a table from the source CSV data\n",
        "CREATE OR REPLACE TABLE orders_raw\n",
        "USING CSV\n",
        "OPTIONS (path \"/path/to/orders.csv\", header \"true\");\n",
        "\n",
        "# 2: Transform the data by adding TotalAmount and filtering\n",
        "CREATE OR REPLACE TABLE orders_transformed AS\n",
        "SELECT *, Quantity * Price AS TotalAmount\n",
        "FROM orders_raw\n",
        "WHERE Quantity > 1;\n",
        "\n",
        "# 3: Write the transformed data into a Delta table\n",
        "CREATE OR REPLACE TABLE delta_orders_table AS\n",
        "SELECT * FROM orders_transformed;\n"
      ],
      "metadata": {
        "id": "jZRjuDTvaMGr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 3: Perform Read, Write, Update, and Delete Operations on Delta Table\n",
        "# 1. Reading the data from the Delta table (PySpark)\n",
        "df = spark.read.format(\"delta\").load(\"/path/to/delta_orders_table\")\n",
        "df.show()\n",
        "\n",
        "# 2. Update the table (Increase the price of laptops by 10%)\n",
        "spark.sql(\"\"\"\n",
        "    UPDATE delta_orders_table\n",
        "    SET Price = Price * 1.10\n",
        "    WHERE Product = 'Laptop'\n",
        "\"\"\")\n",
        "\n",
        "# 3. Delete rows where quantity is less than 2\n",
        "spark.sql(\"\"\"\n",
        "    DELETE FROM delta_orders_table\n",
        "    WHERE Quantity < 2\n",
        "\"\"\")\n",
        "\n",
        "# 4. Insert a new record\n",
        "spark.sql(\"\"\"\n",
        "    INSERT INTO delta_orders_table (OrderID, OrderDate, CustomerID, Product, Quantity, Price, TotalAmount)\n",
        "    VALUES (106, '2024-01-06', 'C006', 'Keyboard', 3, 50, 150)\n",
        "\"\"\")\n"
      ],
      "metadata": {
        "id": "LZfNISBVaNjV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Task 4: Merge Data (SCD Type 2)\n",
        "spark.sql(\"\"\"MERGE INTO delta_orders_table AS target\n",
        "USING (SELECT * FROM new_orders_data) AS source\n",
        "ON target.OrderID = source.OrderID\n",
        "WHEN MATCHED THEN\n",
        "  UPDATE SET target.Quantity = source.Quantity,\n",
        "             target.Price = source.Price,\n",
        "             target.TotalAmount = source.Quantity * source.Price\n",
        "WHEN NOT MATCHED THEN\n",
        "  INSERT (OrderID, OrderDate, CustomerID, Product, Quantity, Price, TotalAmount)\n",
        "  VALUES (source.OrderID, source.OrderDate, source.CustomerID, source.Product, source.Quantity, source.Price, source.Quantity * source.Price)\n",
        "  \"\"\")\n"
      ],
      "metadata": {
        "id": "5I-7MZ_2aN_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Task 5: Explore Delta Table Internals\n",
        "# View the transaction history of the Delta table\n",
        "history_df = spark.sql(\"DESCRIBE HISTORY delta_orders_table\")\n",
        "history_df.show(truncate=False)\n",
        "\n",
        "# View file size and modification times\n",
        "detail_df = spark.sql(\"DESCRIBE DETAIL delta_orders_table\")\n",
        "detail_df.show(truncate=False)"
      ],
      "metadata": {
        "id": "r0q3s5aJahDR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Task 6: Time Travel in Delta Tables\n",
        "# Query the table at a previous version\n",
        "df_time_travel = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(delta_table_path)\n",
        "df_time_travel.show(truncate=False)\n",
        "\n",
        "# Query the table using a timestamp\n",
        "SELECT * FROM delta_orders_table TIMESTAMP AS OF '2024-01-10 00:00:00';"
      ],
      "metadata": {
        "id": "uncG8SndahlV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 7: Optimize Delta Table\n",
        "# Optimize the Delta table using Z-order on the Product column\n",
        "spark.sql(\"OPTIMIZE delta_orders_table ZORDER BY (Product)\")\n",
        "\n",
        "# Vacuum the table to remove old files (older than 7 days)\n",
        "spark.sql(\"VACUUM delta_orders_table RETAIN 7 HOURS\")\n",
        "\n",
        "# Task 8: Converting Parquet Files to Delta Format\n",
        "# Read the Parquet file\n",
        "parquet_df = spark.read.format(\"parquet\").load(\"/path/to/historical_orders_parquet/\")\n",
        "\n",
        "# Convert the Parquet file to Delta\n",
        "parquet_df.write.format(\"delta\").save(\"/path/to/historical_orders_delta/\")\n",
        "\n",
        "# Verify by querying the Delta table\n",
        "delta_df = spark.read.format(\"delta\").load(\"/path/to/historical_orders_delta/\")\n",
        "delta_df.show()"
      ],
      "metadata": {
        "id": "c23SYIOWanva"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Assignment: Creating and Scheduling a Job on Databricks using Notebooks\n"
      ],
      "metadata": {
        "id": "WEMr4T7RbB3z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "from delta import *\n",
        "\n",
        "# Initialize SparkSession\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "# Read the CSV file\n",
        "df = spark.read.csv(\"file:/content/sample_data/orders_data.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# Perform transformation\n",
        "transformed_df = df.withColumn(\"TotalAmount\", col(\"Quantity\") * col(\"Price\")) \\\n",
        "                   .filter(col(\"Quantity\") > 5)\n",
        "\n",
        "# Write the transformed data to a Delta table\n",
        "transformed_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"transformed_orders\")\n",
        "\n",
        "# Read from the Delta table\n",
        "df = spark.table(\"transformed_orders\")\n",
        "\n",
        "# Perform aggregation\n",
        "aggregated_df = df.groupBy(\"Product\").agg({\"Quantity\": \"sum\"})\n",
        "\n",
        "# Write the aggregated data to a Delta table\n",
        "aggregated_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"aggregated_orders\")"
      ],
      "metadata": {
        "id": "Eg8z24btbZ_s"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}