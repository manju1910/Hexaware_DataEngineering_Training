{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "P-d-4NqeHT6b",
        "outputId": "cba1bfd9-0ca8-4165-beeb-08411ef50922"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.2.tar.gz (317.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.2-py2.py3-none-any.whl size=317812365 sha256=8b90712b2579419a3db7f2005fdecec6ee5f8d16693c73354287f780f4080500\n",
            "  Stored in directory: /root/.cache/pip/wheels/34/34/bd/03944534c44b677cd5859f248090daa9fb27b3c8f8e5f49574\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "######################################             E-commerce Transactions          #############################################################33"
      ],
      "metadata": {
        "id": "QY8wZyaZ1Tpn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "omrc43hxGSrX",
        "outputId": "483506cf-d26d-4db1-a586-26162df5fe88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+-----------+------------+--------------+-----+--------+-------------------+----------------+\n",
            "|transaction_id|customer_id|     product|      category|price|quantity|discount_percentage|transaction_date|\n",
            "+--------------+-----------+------------+--------------+-----+--------+-------------------+----------------+\n",
            "|             1|        101|      Laptop|   Electronics| 1000|       1|                 10|      2023-08-01|\n",
            "|             2|        102|  Smartphone|   Electronics|  700|       2|                  5|      2023-08-01|\n",
            "|             3|        103|       Shirt|       Fashion|   40|       3|                  0|      2023-08-02|\n",
            "|             4|        104|     Blender|Home Appliance|  150|       1|                 15|      2023-08-03|\n",
            "|             5|        101|  Headphones|   Electronics|  100|       2|                 10|      2023-08-03|\n",
            "|             6|        105|       Shoes|       Fashion|   60|       1|                 20|      2023-08-04|\n",
            "|             7|        106|Refrigerator|Home Appliance|  800|       1|                 25|      2023-08-05|\n",
            "|             8|        107|        Book|         Books|   20|       4|                  0|      2023-08-05|\n",
            "|             9|        108|     Toaster|Home Appliance|   30|       1|                  5|      2023-08-06|\n",
            "|            10|        102|      Tablet|   Electronics|  300|       2|                 10|      2023-08-06|\n",
            "+--------------+-----------+------------+--------------+-----+--------+-------------------+----------------+\n",
            "\n",
            "root\n",
            " |-- transaction_id: long (nullable = true)\n",
            " |-- customer_id: long (nullable = true)\n",
            " |-- product: string (nullable = true)\n",
            " |-- category: string (nullable = true)\n",
            " |-- price: long (nullable = true)\n",
            " |-- quantity: long (nullable = true)\n",
            " |-- discount_percentage: long (nullable = true)\n",
            " |-- transaction_date: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "\n",
        "spark=SparkSession.builder.appName(\" E-commerce Transactions\").getOrCreate()\n",
        "\n",
        "transactions=[(1,101,'Laptop','Electronics',1000,1,10,'2023-08-01'),\n",
        "(2,102,'Smartphone','Electronics',700,2,5,'2023-08-01'),\n",
        "(3,103,'Shirt','Fashion',40,3,0,'2023-08-02'),\n",
        "(4,104,'Blender','Home Appliance',150,1,15,'2023-08-03'),\n",
        "(5,101,'Headphones','Electronics',100,2,10,'2023-08-03'),\n",
        "(6,105,'Shoes','Fashion',60,1,20,'2023-08-04'),\n",
        "(7,106,'Refrigerator','Home Appliance',800,1,25,'2023-08-05'),\n",
        "(8,107,'Book','Books',20,4,0,'2023-08-05'),\n",
        "(9,108,'Toaster','Home Appliance',30,1,5,'2023-08-06'),\n",
        "(10,102,'Tablet','Electronics',300,2,10,'2023-08-06'),\n",
        "]\n",
        "\n",
        "transaction_columns=['transaction_id','customer_id','product','category','price','quantity','discount_percentage','transaction_date']\n",
        "transactions_df=spark.createDataFrame(transactions,schema=transaction_columns)\n",
        "transactions_df.show()\n",
        "transactions_df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#1. Calculate the Total Revenue per Category\n",
        "\n",
        "Total_revenue=transactions_df.withColumn(\"revenue\",(100-(col(\"discount_percentage\")/100))*col(\"price\")).groupBy(\"category\").sum(\"revenue\")\n",
        "Total_revenue.show()\n",
        "\n",
        "#2. Filter Transactions with a Discount Greater Than 10%\n",
        "\n",
        "filter_data=transactions_df.filter(transactions_df.discount_percentage>10)\n",
        "filter_data.show()\n",
        "\n",
        "#3. Find the Most Expensive Product Sold\n",
        "\n",
        "max_price=transactions_df.orderBy(desc(\"price\")).limit(1)\n",
        "max_price.show()\n",
        "\n",
        "# 4.average price of products per category\n",
        "\n",
        "average_price=transactions_df.groupBy(\"category\").avg(\"price\")\n",
        "average_price.show()\n",
        "\n",
        "# 5.Who bougth more than one product\n",
        "multiple_transactions=transactions_df.groupBy(\"customer_id\").count().filter(col(\"count\")>1)\n",
        "multiple_transactions.show()\n",
        "\n",
        "#6. Find the Top 3 Highest Revenue Transactions\n",
        "Total_revenue=transactions_df.withColumn(\"revenue\",col(\"price\")*col(\"quantity\"))\n",
        "top_three=Total_revenue.orderBy(desc(\"revenue\")).limit(3)\n",
        "top_three.show()\n",
        "\n",
        "#7. Calculate the Total Number of Transactions per Day\n",
        "total_transactions_per_day=transactions_df.groupBy(\"transaction_date\").count()\n",
        "total_transactions_per_day.show()\n",
        "\n",
        "#8. Customer Who spent most money\n",
        "revenue=transactions_df.withColumn(\"revenue\",col(\"quantity\")*col(\"price\"))\n",
        "Valuable_customer_revenue=revenue.groupBy(\"Customer_id\").agg(sum(\"revenue\").alias(\"customer_revenue\"))\n",
        "Valuable_customer=Valuable_customer_revenue.orderBy(desc(\"customer_revenue\")).limit(1)\n",
        "Valuable_customer.show()\n",
        "\n",
        "#9. Calculate the Average Discount Given per Product Category\n",
        "discount_products=transactions_df.groupBy(\"category\").avg(\"discount_percentage\")\n",
        "discount_products.show()\n",
        "\n",
        "#10. Create a New Column for Final Price After Discount\n",
        "final_price=transactions_df.withColumn(\"final_price\", ( col(\"price\") - (col('price') * col('discount_percentage') / 100) ))\n",
        "final_price.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "KV5Q3eY9NQgq",
        "outputId": "20d4e62d-21e5-4d90-eb1d-90db13c43920"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+------------+\n",
            "|      category|sum(revenue)|\n",
            "+--------------+------------+\n",
            "|       Fashion|      9988.0|\n",
            "|   Electronics|    209825.0|\n",
            "|Home Appliance|     97776.0|\n",
            "|         Books|      2000.0|\n",
            "+--------------+------------+\n",
            "\n",
            "+--------------+-----------+------------+--------------+-----+--------+-------------------+----------------+\n",
            "|transaction_id|customer_id|     product|      category|price|quantity|discount_percentage|transaction_date|\n",
            "+--------------+-----------+------------+--------------+-----+--------+-------------------+----------------+\n",
            "|             4|        104|     Blender|Home Appliance|  150|       1|                 15|      2023-08-03|\n",
            "|             6|        105|       Shoes|       Fashion|   60|       1|                 20|      2023-08-04|\n",
            "|             7|        106|Refrigerator|Home Appliance|  800|       1|                 25|      2023-08-05|\n",
            "+--------------+-----------+------------+--------------+-----+--------+-------------------+----------------+\n",
            "\n",
            "+--------------+-----------+-------+-----------+-----+--------+-------------------+----------------+\n",
            "|transaction_id|customer_id|product|   category|price|quantity|discount_percentage|transaction_date|\n",
            "+--------------+-----------+-------+-----------+-----+--------+-------------------+----------------+\n",
            "|             1|        101| Laptop|Electronics| 1000|       1|                 10|      2023-08-01|\n",
            "+--------------+-----------+-------+-----------+-----+--------+-------------------+----------------+\n",
            "\n",
            "+--------------+-----------------+\n",
            "|      category|       avg(price)|\n",
            "+--------------+-----------------+\n",
            "|       Fashion|             50.0|\n",
            "|   Electronics|            525.0|\n",
            "|Home Appliance|326.6666666666667|\n",
            "|         Books|             20.0|\n",
            "+--------------+-----------------+\n",
            "\n",
            "+-----------+-----+\n",
            "|customer_id|count|\n",
            "+-----------+-----+\n",
            "|        101|    2|\n",
            "|        102|    2|\n",
            "+-----------+-----+\n",
            "\n",
            "+--------------+-----------+------------+--------------+-----+--------+-------------------+----------------+-------+\n",
            "|transaction_id|customer_id|     product|      category|price|quantity|discount_percentage|transaction_date|revenue|\n",
            "+--------------+-----------+------------+--------------+-----+--------+-------------------+----------------+-------+\n",
            "|             2|        102|  Smartphone|   Electronics|  700|       2|                  5|      2023-08-01|   1400|\n",
            "|             1|        101|      Laptop|   Electronics| 1000|       1|                 10|      2023-08-01|   1000|\n",
            "|             7|        106|Refrigerator|Home Appliance|  800|       1|                 25|      2023-08-05|    800|\n",
            "+--------------+-----------+------------+--------------+-----+--------+-------------------+----------------+-------+\n",
            "\n",
            "+----------------+-----+\n",
            "|transaction_date|count|\n",
            "+----------------+-----+\n",
            "|      2023-08-01|    2|\n",
            "|      2023-08-02|    1|\n",
            "|      2023-08-03|    2|\n",
            "|      2023-08-06|    2|\n",
            "|      2023-08-04|    1|\n",
            "|      2023-08-05|    2|\n",
            "+----------------+-----+\n",
            "\n",
            "+-----------+----------------+\n",
            "|Customer_id|customer_revenue|\n",
            "+-----------+----------------+\n",
            "|        102|            2000|\n",
            "+-----------+----------------+\n",
            "\n",
            "+--------------+------------------------+\n",
            "|      category|avg(discount_percentage)|\n",
            "+--------------+------------------------+\n",
            "|       Fashion|                    10.0|\n",
            "|   Electronics|                    8.75|\n",
            "|Home Appliance|                    15.0|\n",
            "|         Books|                     0.0|\n",
            "+--------------+------------------------+\n",
            "\n",
            "+--------------+-----------+------------+--------------+-----+--------+-------------------+----------------+-----------+\n",
            "|transaction_id|customer_id|     product|      category|price|quantity|discount_percentage|transaction_date|final_price|\n",
            "+--------------+-----------+------------+--------------+-----+--------+-------------------+----------------+-----------+\n",
            "|             1|        101|      Laptop|   Electronics| 1000|       1|                 10|      2023-08-01|      900.0|\n",
            "|             2|        102|  Smartphone|   Electronics|  700|       2|                  5|      2023-08-01|      665.0|\n",
            "|             3|        103|       Shirt|       Fashion|   40|       3|                  0|      2023-08-02|       40.0|\n",
            "|             4|        104|     Blender|Home Appliance|  150|       1|                 15|      2023-08-03|      127.5|\n",
            "|             5|        101|  Headphones|   Electronics|  100|       2|                 10|      2023-08-03|       90.0|\n",
            "|             6|        105|       Shoes|       Fashion|   60|       1|                 20|      2023-08-04|       48.0|\n",
            "|             7|        106|Refrigerator|Home Appliance|  800|       1|                 25|      2023-08-05|      600.0|\n",
            "|             8|        107|        Book|         Books|   20|       4|                  0|      2023-08-05|       20.0|\n",
            "|             9|        108|     Toaster|Home Appliance|   30|       1|                  5|      2023-08-06|       28.5|\n",
            "|            10|        102|      Tablet|   Electronics|  300|       2|                 10|      2023-08-06|      270.0|\n",
            "+--------------+-----------+------------+--------------+-----+--------+-------------------+----------------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###################################################       Banking Transactions ###########################################################"
      ],
      "metadata": {
        "id": "VIQQjyqN20qe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import *\n",
        "\n",
        "spark=SparkSession.builder.appName(\"Banking Transactions\").getOrCreate()\n",
        "data = [\n",
        "    (1, 201, \"Deposit\", 5000, \"2023-09-01\"),\n",
        "    (2, 202, \"Withdrawal\", 2000, \"2023-09-01\"),\n",
        "    (3, 203, \"Deposit\", 3000, \"2023-09-02\"),\n",
        "    (4, 201, \"Withdrawal\", 1500, \"2023-09-02\"),\n",
        "    (5, 204, \"Deposit\", 10000, \"2023-09-03\"),\n",
        "    (6, 205, \"Withdrawal\", 500, \"2023-09-03\"),\n",
        "    (7, 202, \"Deposit\", 2500, \"2023-09-04\"),\n",
        "    (8, 206, \"Withdrawal\", 700, \"2023-09-04\"),\n",
        "    (9, 203, \"Deposit\", 4000, \"2023-09-05\"),\n",
        "    (10, 204, \"Withdrawal\", 3000, \"2023-09-05\")\n",
        "]\n",
        "columns1=[\"transaction_id\",\"customer_id\",\"transaction_type\",\"amount\",\"transaction_date\"]\n",
        "banking_df=spark.createDataFrame(data,schema=columns1)\n",
        "banking_df.show()\n",
        "banking_df.printSchema()\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "HA4Yc9dyZpCm",
        "outputId": "c5322d63-10c1-4a90-b9cc-e4cf94470a42"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+-----------+----------------+------+----------------+\n",
            "|transaction_id|customer_id|transaction_type|amount|transaction_date|\n",
            "+--------------+-----------+----------------+------+----------------+\n",
            "|             1|        201|         Deposit|  5000|      2023-09-01|\n",
            "|             2|        202|      Withdrawal|  2000|      2023-09-01|\n",
            "|             3|        203|         Deposit|  3000|      2023-09-02|\n",
            "|             4|        201|      Withdrawal|  1500|      2023-09-02|\n",
            "|             5|        204|         Deposit| 10000|      2023-09-03|\n",
            "|             6|        205|      Withdrawal|   500|      2023-09-03|\n",
            "|             7|        202|         Deposit|  2500|      2023-09-04|\n",
            "|             8|        206|      Withdrawal|   700|      2023-09-04|\n",
            "|             9|        203|         Deposit|  4000|      2023-09-05|\n",
            "|            10|        204|      Withdrawal|  3000|      2023-09-05|\n",
            "+--------------+-----------+----------------+------+----------------+\n",
            "\n",
            "root\n",
            " |-- transaction_id: long (nullable = true)\n",
            " |-- customer_id: long (nullable = true)\n",
            " |-- transaction_type: string (nullable = true)\n",
            " |-- amount: long (nullable = true)\n",
            " |-- transaction_date: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#1. Calculate the Total Deposit and Withdrawal Amounts\n",
        "\n",
        "total_grouped_data=banking_df.groupBy(\"transaction_type\").sum(\"amount\")\n",
        "total_grouped_data.show()\n",
        "\n",
        "#2. Filter Transactions Greater Than $3,000\n",
        "filter2=banking_df.filter(banking_df.amount>3000)\n",
        "filter2.show()\n",
        "\n",
        "#3. Find the Largest Deposit Made\n",
        "largest_deposit=banking_df.filter(banking_df.transaction_type==\"Deposit\").orderBy(desc(\"amount\")).limit(1)\n",
        "largest_deposit.show()\n",
        "\n",
        "#4. Calculate the Average Withdrawal Amount\n",
        "average_withdrawal=banking_df.groupBy(\"transaction_type\").agg(avg(\"amount\"))\n",
        "average_withdrawal.show()\n",
        "\n",
        "#5. Find Customers Who Made Both Deposits and Withdrawals\n",
        "deposit_df = banking_df.filter(col(\"transaction_type\") == \"Deposit\").select(\"customer_id\").distinct()\n",
        "withdrawal_df = banking_df.filter(col(\"transaction_type\") == \"Withdrawal\").select(\"customer_id\").distinct()\n",
        "customers_both = deposit_df.join(withdrawal_df, on=\"customer_id\", how=\"inner\")\n",
        "customers_both.show()\n",
        "\n",
        "# 6. Calculate the Total Amount of Transactions per Day\n",
        "transactions_per_day=banking_df.groupBy(\"transaction_date\").sum(\"amount\")\n",
        "transactions_per_day.show()\n",
        "\n",
        "#7. Find the Customer with the Highest Total Withdrawal\n",
        "highest_withdrawl=banking_df.filter(banking_df.transaction_type==\"Withdrawal\").groupBy(\"customer_id\").sum(\"amount\").orderBy(desc(\"sum(amount)\")).limit(1)\n",
        "highest_withdrawl.show()\n",
        "\n",
        "# 8. Calculate the Number of Transactions for Each Customer\n",
        "transactions_per_customer=banking_df.groupBy(\"customer_id\").count()\n",
        "transactions_per_customer.show()\n",
        "\n",
        "#9.Find All Transactions That Occurred on the Same Day as a Withdrawal Greater Than $1,000\n",
        "withdrawals_df = banking_df.filter((col(\"transaction_type\") == \"Withdrawal\") & (col(\"amount\") > 1000))\n",
        "dates_with_large_withdrawals = withdrawals_df.select(\"transaction_date\").distinct()\n",
        "all_transactions_on_dates = banking_df.join(dates_with_large_withdrawals, on=\"transaction_date\", how=\"inner\")\n",
        "all_transactions_on_dates.show()\n",
        "\n",
        "#10. Create a New Column to Classify Transactions as \"High\" or \"Low\" Value\n",
        "classifier=banking_df.withColumn(\"transaction_value\", when(col(\"amount\")>=5000,\"High\").otherwise(\"Low\"))\n",
        "classifier.show()\n",
        "#"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "tgogaMHXbZaC",
        "outputId": "5c6e9228-b2dd-4b98-bf64-e67096407578"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------+-----------+\n",
            "|transaction_type|sum(amount)|\n",
            "+----------------+-----------+\n",
            "|         Deposit|      24500|\n",
            "|      Withdrawal|       7700|\n",
            "+----------------+-----------+\n",
            "\n",
            "+--------------+-----------+----------------+------+----------------+\n",
            "|transaction_id|customer_id|transaction_type|amount|transaction_date|\n",
            "+--------------+-----------+----------------+------+----------------+\n",
            "|             1|        201|         Deposit|  5000|      2023-09-01|\n",
            "|             5|        204|         Deposit| 10000|      2023-09-03|\n",
            "|             9|        203|         Deposit|  4000|      2023-09-05|\n",
            "+--------------+-----------+----------------+------+----------------+\n",
            "\n",
            "+--------------+-----------+----------------+------+----------------+\n",
            "|transaction_id|customer_id|transaction_type|amount|transaction_date|\n",
            "+--------------+-----------+----------------+------+----------------+\n",
            "|             5|        204|         Deposit| 10000|      2023-09-03|\n",
            "+--------------+-----------+----------------+------+----------------+\n",
            "\n",
            "+----------------+-----------+\n",
            "|transaction_type|avg(amount)|\n",
            "+----------------+-----------+\n",
            "|         Deposit|     4900.0|\n",
            "|      Withdrawal|     1540.0|\n",
            "+----------------+-----------+\n",
            "\n",
            "+-----------+\n",
            "|customer_id|\n",
            "+-----------+\n",
            "|        202|\n",
            "|        201|\n",
            "|        204|\n",
            "+-----------+\n",
            "\n",
            "+----------------+-----------+\n",
            "|transaction_date|sum(amount)|\n",
            "+----------------+-----------+\n",
            "|      2023-09-01|       7000|\n",
            "|      2023-09-02|       4500|\n",
            "|      2023-09-03|      10500|\n",
            "|      2023-09-05|       7000|\n",
            "|      2023-09-04|       3200|\n",
            "+----------------+-----------+\n",
            "\n",
            "+-----------+-----------+\n",
            "|customer_id|sum(amount)|\n",
            "+-----------+-----------+\n",
            "|        204|       3000|\n",
            "+-----------+-----------+\n",
            "\n",
            "+-----------+-----+\n",
            "|customer_id|count|\n",
            "+-----------+-----+\n",
            "|        202|    2|\n",
            "|        201|    2|\n",
            "|        203|    2|\n",
            "|        204|    2|\n",
            "|        205|    1|\n",
            "|        206|    1|\n",
            "+-----------+-----+\n",
            "\n",
            "+----------------+--------------+-----------+----------------+------+\n",
            "|transaction_date|transaction_id|customer_id|transaction_type|amount|\n",
            "+----------------+--------------+-----------+----------------+------+\n",
            "|      2023-09-01|             1|        201|         Deposit|  5000|\n",
            "|      2023-09-01|             2|        202|      Withdrawal|  2000|\n",
            "|      2023-09-02|             3|        203|         Deposit|  3000|\n",
            "|      2023-09-02|             4|        201|      Withdrawal|  1500|\n",
            "|      2023-09-05|             9|        203|         Deposit|  4000|\n",
            "|      2023-09-05|            10|        204|      Withdrawal|  3000|\n",
            "+----------------+--------------+-----------+----------------+------+\n",
            "\n",
            "+--------------+-----------+----------------+------+----------------+-----------------+\n",
            "|transaction_id|customer_id|transaction_type|amount|transaction_date|transaction_value|\n",
            "+--------------+-----------+----------------+------+----------------+-----------------+\n",
            "|             1|        201|         Deposit|  5000|      2023-09-01|             High|\n",
            "|             2|        202|      Withdrawal|  2000|      2023-09-01|              Low|\n",
            "|             3|        203|         Deposit|  3000|      2023-09-02|              Low|\n",
            "|             4|        201|      Withdrawal|  1500|      2023-09-02|              Low|\n",
            "|             5|        204|         Deposit| 10000|      2023-09-03|             High|\n",
            "|             6|        205|      Withdrawal|   500|      2023-09-03|              Low|\n",
            "|             7|        202|         Deposit|  2500|      2023-09-04|              Low|\n",
            "|             8|        206|      Withdrawal|   700|      2023-09-04|              Low|\n",
            "|             9|        203|         Deposit|  4000|      2023-09-05|              Low|\n",
            "|            10|        204|      Withdrawal|  3000|      2023-09-05|              Low|\n",
            "+--------------+-----------+----------------+------+----------------+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###################################################           Health & Fitness Tracker Data    #########################################################33"
      ],
      "metadata": {
        "id": "Ik4nylda4FAq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType, DateType\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "spark = SparkSession.builder.appName(\"Health & Fitness Tracker Data\").getOrCreate()\n",
        "data1 = [\n",
        "    (1, \"2023-09-01\", 12000, 500, 7.0, \"Cardio\"),\n",
        "    (2, \"2023-09-01\", 8000, 400, 6.5, \"Strength\"),\n",
        "    (3, \"2023-09-01\", 15000, 650, 8.0, \"Yoga\"),\n",
        "    (1, \"2023-09-02\", 10000, 450, 6.0, \"Cardio\"),\n",
        "    (2, \"2023-09-02\", 9500, 500, 7.0, \"Cardio\"),\n",
        "    (3, \"2023-09-02\", 14000, 600, 7.5, \"Strength\"),\n",
        "    (1, \"2023-09-03\", 13000, 550, 8.0, \"Yoga\"),\n",
        "    (2, \"2023-09-03\", 12000, 520, 6.5, \"Yoga\"),\n",
        "    (3, \"2023-09-03\", 16000, 700, 7.0, \"Cardio\")\n",
        "]\n",
        "schema = StructType([\n",
        "    StructField(\"user_id\", IntegerType(), True),\n",
        "    StructField(\"date\", StringType(), True),\n",
        "    StructField(\"steps_count\", IntegerType(), True),\n",
        "    StructField(\"calories_burned\", IntegerType(), True),\n",
        "    StructField(\"hours_of_sleep\", DoubleType(), True),\n",
        "    StructField(\"workout_type\", StringType(), True)\n",
        "])\n",
        "health_df = spark.createDataFrame(data1, schema)\n",
        "health_df.show()\n",
        "health_df.printSchema()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "LeFy87wiggJH",
        "outputId": "d08d936a-dbc4-4df6-ce1e-bb56460d257d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+-----------+---------------+--------------+------------+\n",
            "|user_id|      date|steps_count|calories_burned|hours_of_sleep|workout_type|\n",
            "+-------+----------+-----------+---------------+--------------+------------+\n",
            "|      1|2023-09-01|      12000|            500|           7.0|      Cardio|\n",
            "|      2|2023-09-01|       8000|            400|           6.5|    Strength|\n",
            "|      3|2023-09-01|      15000|            650|           8.0|        Yoga|\n",
            "|      1|2023-09-02|      10000|            450|           6.0|      Cardio|\n",
            "|      2|2023-09-02|       9500|            500|           7.0|      Cardio|\n",
            "|      3|2023-09-02|      14000|            600|           7.5|    Strength|\n",
            "|      1|2023-09-03|      13000|            550|           8.0|        Yoga|\n",
            "|      2|2023-09-03|      12000|            520|           6.5|        Yoga|\n",
            "|      3|2023-09-03|      16000|            700|           7.0|      Cardio|\n",
            "+-------+----------+-----------+---------------+--------------+------------+\n",
            "\n",
            "root\n",
            " |-- user_id: integer (nullable = true)\n",
            " |-- date: string (nullable = true)\n",
            " |-- steps_count: integer (nullable = true)\n",
            " |-- calories_burned: integer (nullable = true)\n",
            " |-- hours_of_sleep: double (nullable = true)\n",
            " |-- workout_type: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#1.Total steps by each user\n",
        "total_steps=health_df.groupBy(\"user_id\").sum(\"steps_count\")\n",
        "total_steps.show()\n",
        "\n",
        "# 2.Filter Days with More Than 10,000 Steps\n",
        "filter3=health_df.filter(health_df.steps_count>10000)\n",
        "filter3.show()\n",
        "\n",
        "#3.Average Calories Burned by Workout Type\n",
        "average_calories=health_df.groupBy(\"workout_type\").avg(\"calories_burned\")\n",
        "average_calories.show()\n",
        "\n",
        "# 4.Identify the Day with the Most Steps for Each User\n",
        "most_steps=health_df.groupBy(\"user_id\",\"date\").sum(\"steps_count\").orderBy(\"sum(steps_count)\",ascending=False)\n",
        "most_steps.show()\n",
        "\n",
        "#5.Find Users Who Burned More Than 600 Calories on Any Day\n",
        "calories_burned=health_df.filter(health_df.calories_burned>600)\n",
        "calories_burned.show()\n",
        "\n",
        "#6.calculate the Average Hours of Sleep per User\n",
        "average_sleep=health_df.groupBy(\"user_id\").avg(\"hours_of_sleep\")\n",
        "average_sleep.show()\n",
        "\n",
        "# 7. Find the Total Calories Burned per Day\n",
        "avg_calories_per_day=health_df.groupBy(\"date\").sum(\"calories_burned\")\n",
        "avg_calories_per_day.show()\n",
        "\n",
        "#8.identify Users Who Did Different Types of Workouts\n",
        "diff_users = health_df.groupBy(\"user_id\").agg(countDistinct(\"workout_type\").alias(\"distinct_workout_types\"))\n",
        "diff_users = diff_users.filter(col(\"distinct_workout_types\") > 1)\n",
        "diff_users.show()\n",
        "\n",
        "#9. Calculate the Total Number of Workouts per User\n",
        "workouts_per_user=health_df.groupBy(\"user_id\").count()\n",
        "workouts_per_user.show()\n",
        "\n",
        "#10.Create a New Column for \"Active\" Days\n",
        "\n",
        "Active_days=health_df.withColumn(\"Activity\", when(col(\"calories_burned\")>10000,\"Active\").otherwise(\"Inactive\"))\n",
        "Active_days.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "nZofJAtOj_ZU",
        "outputId": "a19ef769-db9e-4c17-c633-5287702f565f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------------+\n",
            "|user_id|sum(steps_count)|\n",
            "+-------+----------------+\n",
            "|      1|           35000|\n",
            "|      3|           45000|\n",
            "|      2|           29500|\n",
            "+-------+----------------+\n",
            "\n",
            "+-------+----------+-----------+---------------+--------------+------------+\n",
            "|user_id|      date|steps_count|calories_burned|hours_of_sleep|workout_type|\n",
            "+-------+----------+-----------+---------------+--------------+------------+\n",
            "|      1|2023-09-01|      12000|            500|           7.0|      Cardio|\n",
            "|      3|2023-09-01|      15000|            650|           8.0|        Yoga|\n",
            "|      3|2023-09-02|      14000|            600|           7.5|    Strength|\n",
            "|      1|2023-09-03|      13000|            550|           8.0|        Yoga|\n",
            "|      2|2023-09-03|      12000|            520|           6.5|        Yoga|\n",
            "|      3|2023-09-03|      16000|            700|           7.0|      Cardio|\n",
            "+-------+----------+-----------+---------------+--------------+------------+\n",
            "\n",
            "+------------+--------------------+\n",
            "|workout_type|avg(calories_burned)|\n",
            "+------------+--------------------+\n",
            "|    Strength|               500.0|\n",
            "|        Yoga|   573.3333333333334|\n",
            "|      Cardio|               537.5|\n",
            "+------------+--------------------+\n",
            "\n",
            "+-------+----------+----------------+\n",
            "|user_id|      date|sum(steps_count)|\n",
            "+-------+----------+----------------+\n",
            "|      3|2023-09-03|           16000|\n",
            "|      3|2023-09-01|           15000|\n",
            "|      3|2023-09-02|           14000|\n",
            "|      1|2023-09-03|           13000|\n",
            "|      1|2023-09-01|           12000|\n",
            "|      2|2023-09-03|           12000|\n",
            "|      1|2023-09-02|           10000|\n",
            "|      2|2023-09-02|            9500|\n",
            "|      2|2023-09-01|            8000|\n",
            "+-------+----------+----------------+\n",
            "\n",
            "+-------+----------+-----------+---------------+--------------+------------+\n",
            "|user_id|      date|steps_count|calories_burned|hours_of_sleep|workout_type|\n",
            "+-------+----------+-----------+---------------+--------------+------------+\n",
            "|      3|2023-09-01|      15000|            650|           8.0|        Yoga|\n",
            "|      3|2023-09-03|      16000|            700|           7.0|      Cardio|\n",
            "+-------+----------+-----------+---------------+--------------+------------+\n",
            "\n",
            "+-------+-------------------+\n",
            "|user_id|avg(hours_of_sleep)|\n",
            "+-------+-------------------+\n",
            "|      1|                7.0|\n",
            "|      3|                7.5|\n",
            "|      2|  6.666666666666667|\n",
            "+-------+-------------------+\n",
            "\n",
            "+----------+--------------------+\n",
            "|      date|sum(calories_burned)|\n",
            "+----------+--------------------+\n",
            "|2023-09-01|                1550|\n",
            "|2023-09-02|                1550|\n",
            "|2023-09-03|                1770|\n",
            "+----------+--------------------+\n",
            "\n",
            "+-------+----------------------+\n",
            "|user_id|distinct_workout_types|\n",
            "+-------+----------------------+\n",
            "|      1|                     2|\n",
            "|      3|                     3|\n",
            "|      2|                     3|\n",
            "+-------+----------------------+\n",
            "\n",
            "+-------+-----+\n",
            "|user_id|count|\n",
            "+-------+-----+\n",
            "|      1|    3|\n",
            "|      3|    3|\n",
            "|      2|    3|\n",
            "+-------+-----+\n",
            "\n",
            "+-------+----------+-----------+---------------+--------------+------------+--------+\n",
            "|user_id|      date|steps_count|calories_burned|hours_of_sleep|workout_type|Activity|\n",
            "+-------+----------+-----------+---------------+--------------+------------+--------+\n",
            "|      1|2023-09-01|      12000|            500|           7.0|      Cardio|Inactive|\n",
            "|      2|2023-09-01|       8000|            400|           6.5|    Strength|Inactive|\n",
            "|      3|2023-09-01|      15000|            650|           8.0|        Yoga|Inactive|\n",
            "|      1|2023-09-02|      10000|            450|           6.0|      Cardio|Inactive|\n",
            "|      2|2023-09-02|       9500|            500|           7.0|      Cardio|Inactive|\n",
            "|      3|2023-09-02|      14000|            600|           7.5|    Strength|Inactive|\n",
            "|      1|2023-09-03|      13000|            550|           8.0|        Yoga|Inactive|\n",
            "|      2|2023-09-03|      12000|            520|           6.5|        Yoga|Inactive|\n",
            "|      3|2023-09-03|      16000|            700|           7.0|      Cardio|Inactive|\n",
            "+-------+----------+-----------+---------------+--------------+------------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##########################################################     Music Streaming Data      ################################################333"
      ],
      "metadata": {
        "id": "7g6lcgQO4kxW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType, DateType\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "\n",
        "spark = SparkSession.builder.appName(\"Music Streaming Data\").getOrCreate()\n",
        "data = [\n",
        "    (1, \"Blinding Lights\", \"The Weeknd\", 200, \"2023-09-01 08:15:00\", \"New York\"),\n",
        "    (2, \"Shape of You\", \"Ed Sheeran\", 240, \"2023-09-01 09:20:00\", \"Los Angeles\"),\n",
        "    (3, \"Levitating\", \"Dua Lipa\", 180, \"2023-09-01 10:30:00\", \"London\"),\n",
        "    (1, \"Starboy\", \"The Weeknd\", 220, \"2023-09-01 11:00:00\", \"New York\"),\n",
        "    (2, \"Perfect\", \"Ed Sheeran\", 250, \"2023-09-01 12:15:00\", \"Los Angeles\"),\n",
        "    (3, \"Don't Start Now\", \"Dua Lipa\", 200, \"2023-09-02 08:10:00\", \"London\"),\n",
        "    (1, \"Save Your Tears\", \"The Weeknd\", 210, \"2023-09-02 09:00:00\", \"New York\"),\n",
        "    (2, \"Galway Girl\", \"Ed Sheeran\", 190, \"2023-09-02 10:00:00\", \"Los Angeles\"),\n",
        "    (3, \"New Rules\", \"Dua Lipa\", 230, \"2023-09-02 11:00:00\", \"London\")\n",
        "]\n",
        "columns=[\"user_id\",\"song_title\",\"artist\",\"duration_seconds\",\"streaming_time\",'location']\n",
        "music_df=spark.createDataFrame(data,schema=columns)\n",
        "music_df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "50Z_bpGiql2q",
        "outputId": "6fda54ca-8e38-4efe-e19a-a1ce059434e3"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---------------+----------+----------------+-------------------+-----------+\n",
            "|user_id|     song_title|    artist|duration_seconds|     streaming_time|   location|\n",
            "+-------+---------------+----------+----------------+-------------------+-----------+\n",
            "|      1|Blinding Lights|The Weeknd|             200|2023-09-01 08:15:00|   New York|\n",
            "|      2|   Shape of You|Ed Sheeran|             240|2023-09-01 09:20:00|Los Angeles|\n",
            "|      3|     Levitating|  Dua Lipa|             180|2023-09-01 10:30:00|     London|\n",
            "|      1|        Starboy|The Weeknd|             220|2023-09-01 11:00:00|   New York|\n",
            "|      2|        Perfect|Ed Sheeran|             250|2023-09-01 12:15:00|Los Angeles|\n",
            "|      3|Don't Start Now|  Dua Lipa|             200|2023-09-02 08:10:00|     London|\n",
            "|      1|Save Your Tears|The Weeknd|             210|2023-09-02 09:00:00|   New York|\n",
            "|      2|    Galway Girl|Ed Sheeran|             190|2023-09-02 10:00:00|Los Angeles|\n",
            "|      3|      New Rules|  Dua Lipa|             230|2023-09-02 11:00:00|     London|\n",
            "+-------+---------------+----------+----------------+-------------------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Calculate the Total Listening Time for Each Use\n",
        "listening_time=music_df.groupBy(\"user_id\").sum(\"duration_seconds\")\n",
        "listening_time.show()\n",
        "\n",
        "#2. Filter Songs Streamed for More Than 200 Seconds\n",
        "streamed_songs=music_df.filter(music_df.duration_seconds>200)\n",
        "streamed_songs.show()\n",
        "\n",
        "#3. Find the Most Popular Artist\n",
        "most_popular_artist=music_df.groupBy(\"artist\").count().orderBy(desc(\"count\")).limit(1)\n",
        "most_popular_artist.show()\n",
        "\n",
        "#4 . Identify the Song with the Longest Duration\n",
        "longest_song=music_df.orderBy(desc(\"duration_seconds\")).limit(1)\n",
        "longest_song.show()\n",
        "\n",
        "#5. Calculate the Average streaming time per artist\n",
        "average_listening_time=music_df.groupBy(\"artist\").avg(\"duration_seconds\")\n",
        "average_listening_time.show()\n",
        "\n",
        "#6.calculate the Average Song Duration by Artist\n",
        "avg_song_duration=music_df.groupBy(\"Artist\").avg(\"duration_seconds\")\n",
        "avg_song_duration.show()\n",
        "\n",
        "#7.Find the Top 3 Most Streamed Songs per User\n",
        "top_streamed=music_df.groupBy(\"user_id\",\"song_title\").count().orderBy(desc(\"count\")).limit(3)\n",
        "top_streamed.show()\n",
        "\n",
        "#8.Identify Users Who Streamed Songs from More Than One Artist\n",
        "multiple_songs_listeners=music_df.groupBy(\"user_id\").agg(countDistinct(\"artist\").alias(\"distinct_artists\"))\n",
        "multiple_songs_listeners=multiple_songs_listeners.filter(col(\"distinct_artists\")>1)\n",
        "multiple_songs_listeners.show()\n",
        "\n",
        "#9. Calculate the Total Streams for Each Location\n",
        "total_streams=music_df.groupBy(\"location\").count()\n",
        "total_streams.show()\n",
        "\n",
        "#10. Create a New Column to Classify Long and Short Songs\n",
        "classifier=music_df.withColumn(\"song_length\", when(col(\"duration_seconds\")>=200,\"Long\").otherwise(\"Short\"))\n",
        "classifier.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "CLcRMAJKrbn1",
        "outputId": "5ad8d773-0fc8-4b1c-9e3c-fc9d5048ea3f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---------------------+\n",
            "|user_id|sum(duration_seconds)|\n",
            "+-------+---------------------+\n",
            "|      1|                  630|\n",
            "|      3|                  610|\n",
            "|      2|                  680|\n",
            "+-------+---------------------+\n",
            "\n",
            "+-------+---------------+----------+----------------+-------------------+-----------+\n",
            "|user_id|     song_title|    artist|duration_seconds|     streaming_time|   location|\n",
            "+-------+---------------+----------+----------------+-------------------+-----------+\n",
            "|      2|   Shape of You|Ed Sheeran|             240|2023-09-01 09:20:00|Los Angeles|\n",
            "|      1|        Starboy|The Weeknd|             220|2023-09-01 11:00:00|   New York|\n",
            "|      2|        Perfect|Ed Sheeran|             250|2023-09-01 12:15:00|Los Angeles|\n",
            "|      1|Save Your Tears|The Weeknd|             210|2023-09-02 09:00:00|   New York|\n",
            "|      3|      New Rules|  Dua Lipa|             230|2023-09-02 11:00:00|     London|\n",
            "+-------+---------------+----------+----------------+-------------------+-----------+\n",
            "\n",
            "+--------+-----+\n",
            "|  artist|count|\n",
            "+--------+-----+\n",
            "|Dua Lipa|    3|\n",
            "+--------+-----+\n",
            "\n",
            "+-------+----------+----------+----------------+-------------------+-----------+\n",
            "|user_id|song_title|    artist|duration_seconds|     streaming_time|   location|\n",
            "+-------+----------+----------+----------------+-------------------+-----------+\n",
            "|      2|   Perfect|Ed Sheeran|             250|2023-09-01 12:15:00|Los Angeles|\n",
            "+-------+----------+----------+----------------+-------------------+-----------+\n",
            "\n",
            "+----------+---------------------+\n",
            "|    artist|avg(duration_seconds)|\n",
            "+----------+---------------------+\n",
            "|  Dua Lipa|   203.33333333333334|\n",
            "|Ed Sheeran|   226.66666666666666|\n",
            "|The Weeknd|                210.0|\n",
            "+----------+---------------------+\n",
            "\n",
            "+----------+---------------------+\n",
            "|    Artist|avg(duration_seconds)|\n",
            "+----------+---------------------+\n",
            "|  Dua Lipa|   203.33333333333334|\n",
            "|Ed Sheeran|   226.66666666666666|\n",
            "|The Weeknd|                210.0|\n",
            "+----------+---------------------+\n",
            "\n",
            "+-------+---------------+-----+\n",
            "|user_id|     song_title|count|\n",
            "+-------+---------------+-----+\n",
            "|      1|Blinding Lights|    1|\n",
            "|      3|     Levitating|    1|\n",
            "|      2|   Shape of You|    1|\n",
            "+-------+---------------+-----+\n",
            "\n",
            "+-------+----------------+\n",
            "|user_id|distinct_artists|\n",
            "+-------+----------------+\n",
            "+-------+----------------+\n",
            "\n",
            "+-----------+-----+\n",
            "|   location|count|\n",
            "+-----------+-----+\n",
            "|Los Angeles|    3|\n",
            "|     London|    3|\n",
            "|   New York|    3|\n",
            "+-----------+-----+\n",
            "\n",
            "+-------+---------------+----------+----------------+-------------------+-----------+-----------+\n",
            "|user_id|     song_title|    artist|duration_seconds|     streaming_time|   location|song_length|\n",
            "+-------+---------------+----------+----------------+-------------------+-----------+-----------+\n",
            "|      1|Blinding Lights|The Weeknd|             200|2023-09-01 08:15:00|   New York|       Long|\n",
            "|      2|   Shape of You|Ed Sheeran|             240|2023-09-01 09:20:00|Los Angeles|       Long|\n",
            "|      3|     Levitating|  Dua Lipa|             180|2023-09-01 10:30:00|     London|      Short|\n",
            "|      1|        Starboy|The Weeknd|             220|2023-09-01 11:00:00|   New York|       Long|\n",
            "|      2|        Perfect|Ed Sheeran|             250|2023-09-01 12:15:00|Los Angeles|       Long|\n",
            "|      3|Don't Start Now|  Dua Lipa|             200|2023-09-02 08:10:00|     London|       Long|\n",
            "|      1|Save Your Tears|The Weeknd|             210|2023-09-02 09:00:00|   New York|       Long|\n",
            "|      2|    Galway Girl|Ed Sheeran|             190|2023-09-02 10:00:00|Los Angeles|      Short|\n",
            "|      3|      New Rules|  Dua Lipa|             230|2023-09-02 11:00:00|     London|       Long|\n",
            "+-------+---------------+----------+----------------+-------------------+-----------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "######################################################          Retail Store Sales Data        ############################################################"
      ],
      "metadata": {
        "id": "Z19DQr_F4x5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType, DateType\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Initialize Spark session\n",
        "spark = SparkSession.builder.appName(\"Retail Store Sales Data\").getOrCreate()\n",
        "\n",
        "# Define the data as a list of tuples\n",
        "data = [\n",
        "    (1, \"Apple\", \"Groceries\", 0.50, 10, \"2023-09-01\"),\n",
        "    (2, \"T-shirt\", \"Clothing\", 15.00, 2, \"2023-09-01\"),\n",
        "    (3, \"Notebook\", \"Stationery\", 2.00, 5, \"2023-09-02\"),\n",
        "    (4, \"Banana\", \"Groceries\", 0.30, 12, \"2023-09-02\"),\n",
        "    (5, \"Laptop\", \"Electronics\", 800.00, 1, \"2023-09-03\"),\n",
        "    (6, \"Pants\", \"Clothing\", 25.00, 3, \"2023-09-03\"),\n",
        "    (7, \"Headphones\", \"Electronics\", 100.00, 2, \"2023-09-04\"),\n",
        "    (8, \"Pen\", \"Stationery\", 1.00, 10, \"2023-09-04\"),\n",
        "    (9, \"Orange\", \"Groceries\", 0.60, 8, \"2023-09-05\"),\n",
        "    (10, \"Sneakers\", \"Clothing\", 50.00, 1, \"2023-09-05\")\n",
        "]\n",
        "\n",
        "# Define the schema for the DataFrame\n",
        "schema = StructType([\n",
        "    StructField(\"transaction_id\", IntegerType(), True),\n",
        "    StructField(\"product_name\", StringType(), True),\n",
        "    StructField(\"category\", StringType(), True),\n",
        "    StructField(\"price\", DoubleType(), True),\n",
        "    StructField(\"quantity\", IntegerType(), True),\n",
        "    StructField(\"sales_date\", StringType(), True)  # Keeping Date as String initially\n",
        "])\n",
        "sales_df = spark.createDataFrame(data, schema)\n",
        "sales_df.show()\n",
        "sales_df.printSchema()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "2BTKG4c0tj07",
        "outputId": "c3a08098-d1c7-4b4b-92fe-b3f46c421e44"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+------------+-----------+-----+--------+----------+\n",
            "|transaction_id|product_name|   category|price|quantity|sales_date|\n",
            "+--------------+------------+-----------+-----+--------+----------+\n",
            "|             1|       Apple|  Groceries|  0.5|      10|2023-09-01|\n",
            "|             2|     T-shirt|   Clothing| 15.0|       2|2023-09-01|\n",
            "|             3|    Notebook| Stationery|  2.0|       5|2023-09-02|\n",
            "|             4|      Banana|  Groceries|  0.3|      12|2023-09-02|\n",
            "|             5|      Laptop|Electronics|800.0|       1|2023-09-03|\n",
            "|             6|       Pants|   Clothing| 25.0|       3|2023-09-03|\n",
            "|             7|  Headphones|Electronics|100.0|       2|2023-09-04|\n",
            "|             8|         Pen| Stationery|  1.0|      10|2023-09-04|\n",
            "|             9|      Orange|  Groceries|  0.6|       8|2023-09-05|\n",
            "|            10|    Sneakers|   Clothing| 50.0|       1|2023-09-05|\n",
            "+--------------+------------+-----------+-----+--------+----------+\n",
            "\n",
            "root\n",
            " |-- transaction_id: integer (nullable = true)\n",
            " |-- product_name: string (nullable = true)\n",
            " |-- category: string (nullable = true)\n",
            " |-- price: double (nullable = true)\n",
            " |-- quantity: integer (nullable = true)\n",
            " |-- sales_date: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 1. Calculate the Total Revenue per Category\n",
        "total_revenue=sales_df.withColumn(\"rev\",col('price')*col(\"quantity\"))\n",
        "total_revenue=total_revenue.groupBy(\"category\").agg(sum(\"rev\").alias(\"total_revenue\"))\n",
        "total_revenue.show()\n",
        "\n",
        "# 2. Filter Transactions Where the Total Sales Amount is Greater Than $100\n",
        "filter4=total_revenue.filter(total_revenue.total_revenue>100)\n",
        "filter4.show()\n",
        "\n",
        "#3. Find the Most Sold Product\n",
        "most_expensive_product=sales_df.groupBy(\"category\").max(\"quantity\")\n",
        "most_expensive_product.show()\n",
        "\n",
        "#4. Top 3 grossers\n",
        "top_three_grossers=sales_df.groupBy(\"product_name\").sum(\"price\")\n",
        "top_three_grossers.show()\n",
        "\n",
        "#5. Calculate the Average Price per Product Category\n",
        "avg_price=sales_df.groupBy(\"category\").avg(\"price\")\n",
        "avg_price.show()\n",
        "\n",
        "#6. Calculate the Total Number of Items Sold per Day\n",
        "total_items_sold=sales_df.groupBy(\"sales_date\").sum(\"quantity\")\n",
        "total_items_sold.show()\n",
        "\n",
        "#7. Identify the Product with the Lowest Price in Each Category\n",
        "lowest_price=sales_df.groupBy(\"category\").min(\"price\")\n",
        "lowest_price.show()\n",
        "\n",
        "# 8. Calculate the Total Revenue for Each Product\n",
        "total_revenue=sales_df.withColumn(\"rev\",col('price')*col(\"quantity\"))\n",
        "total_revenue=total_revenue.groupBy(\"product_name\").agg(sum(\"rev\").alias(\"total_revenue\"))\n",
        "\n",
        "#9. Find the Total Sales per Day for Each Category\n",
        "total_sales_per_day=sales_df.groupBy(\"sales_date\",\"category\").sum(\"quantity\")\n",
        "total_sales_per_day.show()\n",
        "\n",
        "#10. Create a New Column for discounted price\n",
        "discount_price=sales_df.withColumn(\"discounted_price\",col(\"price\")*0.9)\n",
        "discount_price.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "Mpx-aisfu565",
        "outputId": "f99983f1-68f8-4ccf-8c3b-af0531dc08b3"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+------------------+\n",
            "|   category|     total_revenue|\n",
            "+-----------+------------------+\n",
            "| Stationery|              20.0|\n",
            "|  Groceries|13.399999999999999|\n",
            "|Electronics|            1000.0|\n",
            "|   Clothing|             155.0|\n",
            "+-----------+------------------+\n",
            "\n",
            "+-----------+-------------+\n",
            "|   category|total_revenue|\n",
            "+-----------+-------------+\n",
            "|Electronics|       1000.0|\n",
            "|   Clothing|        155.0|\n",
            "+-----------+-------------+\n",
            "\n",
            "+-----------+-------------+\n",
            "|   category|max(quantity)|\n",
            "+-----------+-------------+\n",
            "| Stationery|           10|\n",
            "|  Groceries|           12|\n",
            "|Electronics|            2|\n",
            "|   Clothing|            3|\n",
            "+-----------+-------------+\n",
            "\n",
            "+------------+----------+\n",
            "|product_name|sum(price)|\n",
            "+------------+----------+\n",
            "|     T-shirt|      15.0|\n",
            "|      Banana|       0.3|\n",
            "|      Laptop|     800.0|\n",
            "|    Notebook|       2.0|\n",
            "|       Apple|       0.5|\n",
            "|    Sneakers|      50.0|\n",
            "|      Orange|       0.6|\n",
            "|         Pen|       1.0|\n",
            "|       Pants|      25.0|\n",
            "|  Headphones|     100.0|\n",
            "+------------+----------+\n",
            "\n",
            "+-----------+------------------+\n",
            "|   category|        avg(price)|\n",
            "+-----------+------------------+\n",
            "| Stationery|               1.5|\n",
            "|  Groceries|0.4666666666666666|\n",
            "|Electronics|             450.0|\n",
            "|   Clothing|              30.0|\n",
            "+-----------+------------------+\n",
            "\n",
            "+----------+-------------+\n",
            "|sales_date|sum(quantity)|\n",
            "+----------+-------------+\n",
            "|2023-09-01|           12|\n",
            "|2023-09-02|           17|\n",
            "|2023-09-03|            4|\n",
            "|2023-09-05|            9|\n",
            "|2023-09-04|           12|\n",
            "+----------+-------------+\n",
            "\n",
            "+-----------+----------+\n",
            "|   category|min(price)|\n",
            "+-----------+----------+\n",
            "| Stationery|       1.0|\n",
            "|  Groceries|       0.3|\n",
            "|Electronics|     100.0|\n",
            "|   Clothing|      15.0|\n",
            "+-----------+----------+\n",
            "\n",
            "+----------+-----------+-------------+\n",
            "|sales_date|   category|sum(quantity)|\n",
            "+----------+-----------+-------------+\n",
            "|2023-09-01|  Groceries|           10|\n",
            "|2023-09-02|  Groceries|           12|\n",
            "|2023-09-01|   Clothing|            2|\n",
            "|2023-09-02| Stationery|            5|\n",
            "|2023-09-03|Electronics|            1|\n",
            "|2023-09-05|  Groceries|            8|\n",
            "|2023-09-04| Stationery|           10|\n",
            "|2023-09-04|Electronics|            2|\n",
            "|2023-09-03|   Clothing|            3|\n",
            "|2023-09-05|   Clothing|            1|\n",
            "+----------+-----------+-------------+\n",
            "\n",
            "+--------------+------------+-----------+-----+--------+----------+----------------+\n",
            "|transaction_id|product_name|   category|price|quantity|sales_date|discounted_price|\n",
            "+--------------+------------+-----------+-----+--------+----------+----------------+\n",
            "|             1|       Apple|  Groceries|  0.5|      10|2023-09-01|            0.45|\n",
            "|             2|     T-shirt|   Clothing| 15.0|       2|2023-09-01|            13.5|\n",
            "|             3|    Notebook| Stationery|  2.0|       5|2023-09-02|             1.8|\n",
            "|             4|      Banana|  Groceries|  0.3|      12|2023-09-02|            0.27|\n",
            "|             5|      Laptop|Electronics|800.0|       1|2023-09-03|           720.0|\n",
            "|             6|       Pants|   Clothing| 25.0|       3|2023-09-03|            22.5|\n",
            "|             7|  Headphones|Electronics|100.0|       2|2023-09-04|            90.0|\n",
            "|             8|         Pen| Stationery|  1.0|      10|2023-09-04|             0.9|\n",
            "|             9|      Orange|  Groceries|  0.6|       8|2023-09-05|            0.54|\n",
            "|            10|    Sneakers|   Clothing| 50.0|       1|2023-09-05|            45.0|\n",
            "+--------------+------------+-----------+-----+--------+----------+----------------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}
